{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Code\n",
    "\n",
    "The aim of this is to use the normalised college data to train a model that predicts the likelihood of college acceptance, given several factors. Some factors have to do with the student (e.g. GPA, minority), and some have to do with the college (e.g. acceptance rate, private/public). Together, these predictors should fit a model that returns probability of being accepted by a college. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import TIdatabase as ti\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from matplotlib import rcParams\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Checking\n",
    "\n",
    "We read in the normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studentID</th>\n",
       "      <th>classrank</th>\n",
       "      <th>admissionstest</th>\n",
       "      <th>AP</th>\n",
       "      <th>averageAP</th>\n",
       "      <th>SATsubject</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GPA_w</th>\n",
       "      <th>program</th>\n",
       "      <th>schooltype</th>\n",
       "      <th>...</th>\n",
       "      <th>alumni</th>\n",
       "      <th>outofstate</th>\n",
       "      <th>acceptStatus</th>\n",
       "      <th>acceptProb</th>\n",
       "      <th>name</th>\n",
       "      <th>acceptrate</th>\n",
       "      <th>size</th>\n",
       "      <th>public</th>\n",
       "      <th>finAidPct</th>\n",
       "      <th>instatePct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S50C3UECT8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838909</td>\n",
       "      <td>7</td>\n",
       "      <td>1.067927</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.188104</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>Biomedical engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.151</td>\n",
       "      <td>6621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBWZQQRBEV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666993</td>\n",
       "      <td>7</td>\n",
       "      <td>0.661638</td>\n",
       "      <td>2</td>\n",
       "      <td>0.493061</td>\n",
       "      <td>0.398012</td>\n",
       "      <td>Classics</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.151</td>\n",
       "      <td>6621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MXXLWO1HQ2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.208552</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.395752</td>\n",
       "      <td>-1.035963</td>\n",
       "      <td>Biological Science</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.151</td>\n",
       "      <td>6621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5KSL7C8SLZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.297350</td>\n",
       "      <td>7</td>\n",
       "      <td>0.864783</td>\n",
       "      <td>4</td>\n",
       "      <td>0.103824</td>\n",
       "      <td>-0.384156</td>\n",
       "      <td>Physics</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.151</td>\n",
       "      <td>6621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQWLNGGZ49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.323162</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.354087</td>\n",
       "      <td>2</td>\n",
       "      <td>0.541716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.151</td>\n",
       "      <td>6621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    studentID  classrank  admissionstest  AP  averageAP  SATsubject       GPA  \\\n",
       "0  S50C3UECT8        NaN        0.838909   7   1.067927           3 -0.188104   \n",
       "1  GBWZQQRBEV        NaN        0.666993   7   0.661638           2  0.493061   \n",
       "2  MXXLWO1HQ2        NaN        0.208552   0        NaN           3  0.395752   \n",
       "3  5KSL7C8SLZ        NaN        1.297350   7   0.864783           4  0.103824   \n",
       "4  RQWLNGGZ49        NaN        0.323162   1  -0.354087           2  0.541716   \n",
       "\n",
       "      GPA_w                 program  schooltype     ...      alumni  \\\n",
       "0  0.059072  Biomedical engineering           0     ...           0   \n",
       "1  0.398012                Classics           0     ...           0   \n",
       "2 -1.035963      Biological Science           1     ...           0   \n",
       "3 -0.384156                 Physics           1     ...           0   \n",
       "4       NaN                     NaN           1     ...           0   \n",
       "\n",
       "  outofstate  acceptStatus  acceptProb  name  acceptrate  size  public  \\\n",
       "0          0             1         NaN  Rice       0.151  6621       0   \n",
       "1          1             1         NaN  Rice       0.151  6621       0   \n",
       "2          1             0         NaN  Rice       0.151  6621       0   \n",
       "3          1             0         NaN  Rice       0.151  6621       0   \n",
       "4          1             1         NaN  Rice       0.151  6621       0   \n",
       "\n",
       "   finAidPct  instatePct  \n",
       "0          0           0  \n",
       "1          0           0  \n",
       "2          0           0  \n",
       "3          0           0  \n",
       "4          0           0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"collegedata_normalized.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check how much of the data is missing. The cell below shows that 23% of all entries are null. This is way too high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction nulls =  0.232593074511\n"
     ]
    }
   ],
   "source": [
    "x = df.isnull().sum(axis=1).tolist()\n",
    "y = float(sum(x)) / (df.shape[0]*df.shape[1])\n",
    "print 'Fraction nulls = ', y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease the proportion of NaNs, we get rid of columns which are almost all null. From the `df.head()` above, it appears like some columns have almost exclusively nulls. Therefore the 23% above is probably driven mainly by columns that are basically all null. To see this, we print and plot the % null values for each column. We see the histogram is very bimodal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHkAAAEZCAYAAAADoPklAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X98zfX///H72U/7YeVHzdgiiYVhQyKFEUV+jSJWfmRS\nVH7kV2/yo1KG1VAbw6xC8rv8HlJi5VekH1vK77LIWDZjc3a+f/jufDrNOJszZ47b9XLp8n6f5+t5\nXs/Hzs7Ttvt5vp4vg8lkMgkAAAAAAAC3NCd7FwAAAAAAAIAbR8gDAAAAAADgAAh5AAAAAAAAHAAh\nDwAAAAAAgAMg5AEAAAAAAHAAhDwAAAAAAAAOgJAHAAA7iYyM1IMPPqimTZtqypQpunz5cr4+gwcP\n1vPPP1+o8wYGBqpWrVr68ccfC+wTGhqq1q1bF7pmSZoxY4YCAwO1d+9eSdKJEycUGBioMWPGFPic\nzz///Lp9JOnixYuqX7++HnvsMavrsWb8kiAwMDDffzVr1lSDBg3UtWtXxcfHy2g03pRali9fftV6\n/v1fw4YNb0otR48evSnjAABwO3CxdwEAANyOVq5cqXnz5qlPnz7y8vJSbGysfHx89MILL5j7HDhw\nQBs3btTy5csLfX6j0ajRo0dr+fLlcnV1vWofg8FQ5PoLe742bdrozTffVGJiosaNG1dgTZs3b1Zm\nZqb69+9v0/FLirJly2r06NHmxyaTSefPn9f69es1efJk/fjjj5o2bdpNq6d169YFBmoFfY9s6YUX\nXlBOTo7mzZtX7GMBAHA7IOQBAMAOVq1apbp162rkyJGSpN9//10rV660CHmmTZumtm3bKjAwsEhj\nHDx4UDExMXrllVdsUvONcHd3V9u2bbV48WJ99dVXatWq1VX7rVy5Us7OzurUqdNNrvDm8PDwUPv2\n7fO1d+/eXU8//bTWrFmjvn37qlatWjelnurVq1+1npvlq6++UpMmTew2PgAAjobLtQAAsIO//vpL\nAQEB5scBAQE6efKk+fG2bdu0Z88eDRkypEjnr1Onjvz8/DR79mwlJyffcL220KVLF0nS6tWrr3r8\n77//1vbt29W0aVP5+vrezNLsztnZWU8++aQkaffu3XauBgAA3KoIeQAAsINy5crpn3/+MT8+d+6c\nypYtK+nKJTzTpk3TM888o0qVKhXp/N7e3nrzzTd1+fJljRo16qr7/fzbd999p8DAQMXExOQ7diP7\n9/xbnTp1dP/992vr1q3KzMzMd3z16tXKzc01h0GStGbNGj333HNq1KiRateurYcfflhDhgzRsWPH\nrjnWs88+e9XVMHl70Xz++ecW7d9//70iIiLUsGFD1a1bV507d9aSJUvyPf+bb77Rs88+q4ceekj1\n6tVT+/btNXv2bJvspZN3udm/z2U0GjV//ny1b99ederUUaNGjTRo0CClpKRYPHfUqFFq2LChNm3a\npGbNmqlu3bqaMGHCDdf0b7/99pteffVVNWrUSEFBQWrXrp3i4uLyfe2XLl1SbGysOnfurODgYAUF\nBalVq1Z65513dOHCBUn/936TpB07digwMFArV66UdGXvoj59+uQbf9SoUQoMDNRff/1lcY5PP/1U\nzz33nIKCghQaGqqMjAxJ0smTJzV69Gg1bdpUQUFBeuyxxxQVFaWsrCybvi4AAJQkXK4FAIAdNG7c\nWLGxsdq+fbtKly6tjRs3qk2bNpKkL774QsePH9f8+fNvaIymTZuqS5cuWrZsmWbNmqWBAwde9zkF\n7Wtjq/1uwsLCNHnyZCUmJua7JGvVqlUqW7asQkNDJUlz5szR1KlT9cgjj+jVV1+Vs7Ozdu3apTVr\n1mj//v1KTEyUs7Nzob+W/x5LTEzU4MGDVbVqVQ0YMEDu7u7avHmzxo4dq5SUFPOGznv27NGAAQNU\nu3Ztvfzyy3J1ddWmTZsUFRWlM2fOWOy1UxTbt2+XJAUFBUm6EvYNGTJEiYmJateunXr27Km///5b\nixcvVrdu3TR37lzVr1/f/PysrCy9/vrr6tOnjzw8PHTfffddd8ysrCylpaXla3dyctKdd95pfrxv\n3z716dNHZcqUUZ8+fXTHHXcoKSlJ06ZN0/fff68PPvjA/Jq+8sor2rZtm55++mn16NFDFy5c0IYN\nG5SQkKC///5b06ZNU7Vq1RQZGakRI0bo/vvvV//+/RUcHGwerzDvw8jISDVt2lRvvPGG0tLS5O3t\nrWPHjql79+6SpG7duqlChQrav3+/5syZo6SkJH3yySdyd3e/7usDAMCthpAHAAA76N27t3bu3Gm+\nc1a9evU0ePBgZWdnKzo6Wv369bP4I7uoRo8erW+++UYxMTF67LHHVL169Rs+543o2LGjpk2bptWr\nV1uEPAcPHtQvv/yi3r17y8XFRUajUXPmzFG9evUUFxdn7tetWzeZTCatWbNGv/76qx544IEbqufC\nhQsaM2aMateurYULF5pDo/DwcI0dO1affPKJOnTooDp16mj16tW6fPmyYmJiVKZMGUnS008/rb59\n+1p9hyiTyaSzZ8/KZDKZH58+fVpLly7Vtm3b1KBBA/NdrdauXauNGzdqzJgxCg8PN5+jR48e6tCh\ng9544w2tWbPG3H758mX169evUJtWz507V3Pnzs3X7uPjo507d5prfP3111WuXDmtXLlS3t7ekqRn\nnnlGMTExio6O1rp169S2bVslJyfrq6++Ur9+/fTaa6+ZzxceHq5WrVrpq6++knRlJVuHDh00YsQI\n3XXXXVbvC5T3uv2bn5+fpk+fbtE2YcIEGY1GrVy5Un5+fpKuvHceeughjRgxQh999JEiIiKsGhMA\ngFsJIQ8AAHbg6emp+fPn69ChQzKZTOZVFwkJCcrOzlafPn2Uk5OjqKgobdiwQc7OzurYsaNefPHF\na65e+S9vb29NnDhRL7zwgkaNGqUlS5YU6vm2VrZsWTVv3lxbt25VWlqa+RK1vEt1unbtKunKHjXb\ntm3Ld2nN+fPnVapUKUm66iVfhbVjxw6lp6erdevWSk9PtzjWtm1bLVmyRBs3blSdOnVUsWJFSdL4\n8ePVp08f1alTR05OToW6M9Sff/6pxo0b52t3cXFRu3bt9MYbb5jb1qxZI4PBoJYtW+ZbbdO8eXMt\nXbpUhw4dUtWqVc3tVzv3tXTq1EkdO3bM1/7vO2slJyfr0KFDCg8PV3Z2tkUtjz/+uKKjo5WYmGje\nJHzPnj353mOnT5/WHXfcYb7UypYaNWpk8Tg9PV3bt29Xq1atVKpUKYt6mzZtKk9PTyUmJhLyAAAc\nEiEPAAB29O8/0DMyMhQTE6MhQ4aoVKlSmjlzppYsWaIpU6bIaDRq5MiRcnNzK/TtxZs1a6aOHTtq\n1apVmjNnjsUdvOwhLCxMmzZt0rp169SzZ0/l5ubqiy++UN26dVWtWjVzP1dXV3333Xdav369Dh8+\nrD/++EOpqanm41db1VFYhw8fliRNmTJFU6ZMyXfcYDCYN8QODw/Xzp07tWHDBm3YsEF33HGHGjVq\npNatW+uJJ56wKjwrX768xTjOzs4qXbq0qlatmu/yoSNHjshkMqlFixZXPZfBYNCff/5p8R4qX778\n9b/of/H3979uMJT3Gn3yySf65JNPrtrn35uGu7q66vPPP1dSUpKOHj2q48ePmwO04rjNfbly5Swe\n562q2rRpkzZt2nTV5/z55582rwMAgJKAkAcAgBJizpw5KlOmjJ566ilJV1a3PP744+Y/8p944gkt\nX7680CGPJL3++uvavn27PvjgA7Vs2bJQz718+bLFyo4b1axZM5UvX15ffPGFevbsqW+//VanTp3S\noEGDLPoNHjxY69evV40aNVS3bl09/vjjqlWrlr755ht9+OGHRRr7v5sE5wVFQ4cONe+F8195IYKH\nh4fi4uKUkpKiL7/8UklJSdq6das2btyohIQELVy48Lqvk7u7u9WrbXJzc+Xj46Po6OgC+9SoUcPi\nsZOT7e+pkfcaPffccwUGTl5eXpKubCDevXt3HTt2TA8++KAaNGig7t27q169epo4caJ27dpV5DoK\n2jz8v+Fabm6uJKldu3bmlWH/Zcv3MwAAJQkhDwAAJcDp06eVkJCgyMhI8x/qp0+ftlilUKZMGYuV\nLIVxxx13aPz48Ro0aJBGjx5t/kM4T94fyhcvXrRoz8nJ0blz5+Th4VGkca8m79KzuXPnKjU1VV98\n8YU8PDzUrl07c5/du3dr/fr16ty5s9555x2L569atcqqMYxGo4xGo0UIcPr0aYt+/v7+kqRSpUrl\nC1/S0tK0Z88ec5/jx4/r1KlTql+/vmrUqKEBAwbowoULev3117V+/Xpt375dzZs3L9RrcS3+/v46\nevSoateurdKlS1sc27dvnzIzM236fSlIQECApCurcP77Gl26dElbtmzRXXfdJUlauHChjhw5okmT\nJiksLMyi739f+4I4OTnlex9K0t9//23V8/PuSJednX3VQG39+vXm7ykAAI6GW6gDAFACzJw5UzVq\n1NBjjz1mbqtQoYKOHz9ufnzs2DFVqFChyGO0atVK7dq104EDB/KFRb6+vpKkAwcOWLRv2LBB2dnZ\nRR6zIHkBwPr167Vlyxa1adPGvBpEurIiRJLF5VvSlaBl/fr1MhgM17wtvK+vr0wmk8XXk52drXXr\n1ln0y9ujZf78+Tp//rzFsSlTpujll1/Wjz/+KEl688031bt3b4vXztPT07yZtYuLbT87a9OmjUwm\nU75Nhc+cOaOBAwdq2LBhN2V/pdq1a6tixYpaunRpvsuc4uLiNGTIEG3dulWSdPbsWUnS/fffb9Fv\n06ZN5su+/h0wGgyGfIHj3Xffrd9//90i6Dl+/Lj27t1r1eVed911l0JCQrRlyxbz9y7PihUrNHjw\nYC1btuy65wEA4FZk15U8ubm5SkhI0GeffabU1FRVrFhRPXr0UM+ePSVJP/7441WX2fbt21cjRoy4\n2eUCAFAsjhw5omXLluW7ZXr79u0VGxur6dOny2QyafPmzXrllVduaKwxY8YoKSlJaWlpFnvaBAQE\nqH79+tq+fbtGjhypBg0aKCUlRStWrJC/v79N9r/5t/vuu0/16tVTTEyM0tPT8/28DwkJ0R133KEP\nP/xQGRkZqlixog4dOqRly5bJz89P6enp+UKZf+vUqZNWrVqlIUOGqFevXnJyctKKFSvyBQo+Pj4a\nM2aM/ve//6lDhw566qmnVKZMGX311VfaunWrmjVrZr61ff/+/bVjxw716NFD3bp1U9myZXXw4EF9\n+umnqlmzppo0aWLT1ygsLEzr1q3Txx9/rCNHjqhZs2a6cOGCPv30U509e1aTJ0+Wm5ubTce8Gicn\nJ7355psaMGCAOnfurO7du6tixYras2ePvvjiC9WsWVM9evSQJIWGhurjjz/WsGHD1KNHD3l4eGjP\nnj3asGGD7rvvPv3+++/6559/zHeOK1++vH766SctWrRIDRs2VLVq1dS5c2fFxMSoT58+6tixo86c\nOaNFixYpICBAv//+u1U1jxs3Tj179lR4eLi6d++uqlWr6pdfftGSJUtUqVIlvfTSS8X2egEAYE92\nDXk++OADxcXFaeDAgapbt652796tSZMmKSsrS/369VNycrI8PDyUkJBg8by7777bThUDAGB77733\nnpo2baoGDRpYtPfv31/nz5/X4sWL5ezsrOeff958y/WiKlOmjN544w0NHjw436qI999/X5GRkdq6\ndas2bNigoKAgzZkzR3PmzNFvv/1m7mcwGGyygW6XLl00duxYVa5cOd/XXrZsWc2dO1fTpk3TggUL\nlJOTo5o1a+qtt95SUFCQWrRooe3bt6t169ZXPXfjxo01efJk8znKlSunjh07KjQ0VN26dbPoGxYW\nJj8/P82ZM0fx8fHKyclRQECAhg0bZg6IJKlBgwaaN2+eZs+erY8//ljp6emqUKGCevbsqRdffNHm\n++E4Oztr1qxZio+P1xdffKHIyEh5eXnpgQce0Ntvv21xKVJhvyeF7f/www/r008/VUxMjBYvXqwL\nFy6oYsWKioiIUEREhDw9PSVded2nTZumuLg4RUdHy83NTQ8++KAWL16s5ORkjR49Wjt27FDbtm0l\nSSNGjNDUqVP1zjvv6MUXX1S1atU0cOBA82bcb7/9tqpUqaKhQ4fq/Pnzevfdd62qt0aNGlq+fLlm\nzpyp1atXKz09Xb6+vnr66ac1YMAA8+VlAAA4GoPJ1h/NWcloNOrBBx9Ur169LD6VnDhxotavX68d\nO3bo7bff1oEDB/Tpp5/ao0QAAAAAAIBbht325MnMzFTnzp3zfQJXpUoVpaWlKSsrSykpKebr3AEA\nAAAAAFAwu63kKUifPn105MgRffnll3rooYcUFBSk06dP67ffflPFihX10ksvqVOnTvYuEwAAAAAA\noEQpUbdQX7JkiZKSkjR27FidOnVK586d07FjxzR06FD5+Pho9erVGjVqlCQR9AAAAAAAAPxLiVnJ\n8/nnn2v06NF67LHH9P777+vSpUvau3evqlevrnLlypn7RURE6MiRI0pMTLRjtQAAAAAAACVLiVjJ\nEx8fr8jISLVs2VJTp06VJLm7u1vcNSJP06ZNtW3bNmVlZcnDw8PqMfbs2WOzegEAAAAAAGylfv36\nNjmP3UOeqKgozZ49W507d9bbb79tvv3o4cOHlZSUpK5du8rNzc3c/9KlSypVqlShAp48tnrRAFzZ\nPF2SvLy87FwJ4BiYU4DtMa8A22NeAbaVmZmp5ORkm53PbnfXkqSEhATNnj1bvXr10jvvvGMOeCQp\nNTVVEydO1Ndff21uM5lM2rhxoxo0aGCPcgEAAAAAAEosu63kOXXqlKZOnarq1aurbdu22rdvn8Xx\n+vXrKzg4WOPGjVN6errKly+vzz77TAcPHtSiRYvsVDUAAAAAAEDJZLeQ55tvvlFOTo4OHjyobt26\nWRwzGAxKSkpSTEyMoqKiNH36dJ07d061atXSvHnzVLNmTTtVDQAAAAAAUDLZLeQJCwtTWFjYdftN\nnDjxJlQDAAAAAABwa7PrnjwAAAAAAACwDUIeAAAAAAAAB0DIAwAAAAAA4AAIeQAAAAAAABwAIQ8A\nAAAAAIADIOQBAAAAAABwAIQ8AAAAAAAADoCQBwAAAAAAwAEQ8gAAAAAAADgAQh4AAAAAAAAHQMgD\nAAAAAADgAAh5AAAAAAAAHAAhDwAAAAAAgAMg5AEAAAAAAHAAhDwAAAAAAAAOgJAHAAAAAADAARDy\nAAAAAAAAOABCHgAAAAAAAAfgYu8CbqbY5T/YuwSbatukiu6p4GPvMgAAAAAAQAlQ6JAnOztbbm5u\nkqRz585p48aNcnFx0WOPPabSpUvbvEBbOnX2gr1LsCmTvQsAAAAAAAAlhtUhzz///KOhQ4fqn3/+\n0Weffabz58+rc+fOOnnypCTpvffe08KFCxUQEFBsxQIAAAAAAODqrN6TJyoqSt9++60effRRSdKy\nZct08uRJjRo1Sh9//LGcnZ313nvvFVuhAAAAAAAAKJjVK3m2bNmiZ599VoMGDZIkbdiwQeXLl1ev\nXr1kMBjUo0cPzZ07t9gKBQAAAAAAQMGsXslz7tw5VatWTZKUlpam/fv365FHHpHBYJAk3XHHHbp0\n6VLxVAkAAAAAAIBrsjrkqVChgg4ePChJWrdunXJzcxUaGmo+vmPHDlWqVMn2FQIAAAAAAOC6rL5c\n68knn9SsWbN09OhRffvtt/L19VWzZs107NgxTZo0SVu3btXIkSOLs1YAAAAAAAAUwOqVPC+//LIG\nDhyoo0ePKiQkRLNmzZKbm5suXLig77//XoMGDVLv3r0LNXhubq7i4+P1xBNPKDg4WO3atdOCBQss\n+sTExKh58+aqV6+e+vbtq0OHDhVqDAAAAAAAgNuB1St5DAaDXnrpJb300ksW7TVq1NCOHTvk7Oxc\n6ME/+OADxcXFaeDAgapbt652796tSZMmKSsrS/369dPMmTMVFxen4cOHq2LFioqJiVHv3r21du1a\neXt7F3o8AAAAAAAAR2V1yJNn586d2rp1q1JTUzVgwAB5eHjo+++/1xNPPCFXV1erz2M0GjV//nz1\n69dPL7zwgiTpoYceUlpamubNm6dnnnlGc+fO1csvv6zw8HBJUoMGDdSiRQstXbq00KuGAAAAAAAA\nHJnVl2sZjUYNHTpUzz33nOLj47Vu3TqdOXNGP/30k0aMGKHnnntO58+ft3rgzMxMde7cWa1bt7Zo\nr1KlitLS0vTtt98qKyvLYnNnHx8fNWzYUNu2bbN6HAAAAAAAgNuB1SFPbGys1q1bp7FjxyoxMVEm\nk0mS1LJlS40ZM0YHDhzQzJkzrR7Yx8dHY8aMUWBgoEX7l19+KT8/P6WmpkqS7rnnHovj/v7+Onz4\nsNXjAAAAAAAA3A6sDnlWrFihLl26qGfPnvL09DS3u7q6Kjw8XN27d9emTZtuqJglS5YoKSlJ/fr1\nU0ZGhtzc3OTiYnlFmZeXlzIzM29oHAAAAAAAAEdjdcjz119/KSgoqMDj1apV06lTp4pcyOeff65x\n48bp8ccfV8+ePWUymWQwGK7at6B2AAAAAACA25XVGy9XqFBBKSkpBR7fvXu3KlSoUKQi4uPjFRkZ\nqZYtW2rq1KmSpNKlSys7O1tGo9Hizl2ZmZny8fEp0jjGy8YiPa+kysnOZlUT7CY3N1eSeA8CNsKc\nAmyPeQXYHvMKsK28OWUrVq/kCQsL02effabPP//coohLly5p5syZWr16tdq3b1/oAqKiojR58mR1\n6tRJ06dPN1+eVblyZZlMJp04ccKi/4kTJ3TvvfcWehwAAAAAAABHZvVKnoiICP32228aMWKEOYgZ\nOnSo/vnnHxmNRj366KMaMGBAoQZPSEjQ7Nmz1atXL40ePdriWHBwsNzd3ZWYmKh+/fpJktLT07Vz\n50698sorhRonj7OL8/U73UJc3dzk5eVl7zJwm8r79Ib3IGAbzCnA9phXgO0xrwDbsvWqOKtDHhcX\nF02bNk1du3bVpk2bdOzYMeXm5srPz08tWrRQy5YtCzXwqVOnNHXqVFWvXl1t27bVvn37LI4HBQUp\nPDxc0dHRcnJyUuXKlRUbGysfHx917dq1UGMBAAAAAAA4OqtDntdee02PP/64WrVqpcaNG9/wwN98\n841ycnJ08OBBdevWzeKYwWBQUlKShg4dKicnJ82bN0+ZmZkKCQlRZGSkvL29b3h8AAAAAAAAR2J1\nyLNx40YFBwfbbOCwsDCFhYVdt9+wYcM0bNgwm40LAAAAAADgiKzeeLl69er66aefirMWAAAAAAAA\nFJHVK3k6deqkadOm6eDBg6pfv77Kli0rg8GQr19ERIRNCwQAAAAAAMD1WR3yvPXWW5KkAwcO6MCB\nAwX2I+QBAAAAAAC4+awOeTZt2lScdQAAAAAAAOAGWB3y+Pv7F2cdAAAAAAAAuAFWhzyzZ8++6h48\n/8XlWgAAAAAAADef1SFPVFTUNY+7urrKxcWFkAcAAAAAAMAObmhPntzcXJ05c0br169XYmKiPvro\nI5sWBwAAAAAAAOvc8J4899xzj4KDg3Xu3Dm99dZbio2NtVlxAAAAAAAAsI6TrU5Uv359fffdd7Y6\nHQAAAAAAAArBZiHPt99+q1KlStnqdAAAAAAAACgEqy/XGjdu3FXvrpWdna3k5GT9/PPPCg8Pt2lx\nAAAAAAAAsI7VIc/ixYuv2u7k5KTy5curT58+evXVV21WGAAAAAAAAKxndciTnJxcnHUAAAAAAADg\nBli9J8/MmTP166+/Fnj8hx9+0IQJE2xSFAAAAAAAAAqnUCFPSkpKgce3b9+upUuX2qQoAAAAAAAA\nFE6Bl2sdP35cYWFhys7OlslkkiSNHj1a//vf//L1zc3N1eXLl1WzZs3iqxQAAAAAAAAFKjDkCQgI\n0MiRI7V7925J0sqVK1WvXj35+/vn6+vk5KRy5cqpW7duxVcpAAAAAAAACnTNjZe7du2qrl27SpL+\n+OMPvfjii2rSpMlNKQwAAAAAAADWs/ruWh9//PF1+2RkZMjb2/uGCgIAAAAAAEDhWR3ySNKSJUu0\nY8cOXbhwQbm5ueZ2o9GojIwMJScn64cffrB5kQAAAAAAALg2q0OeOXPmaOrUqXJzc5O3t7fS0tJU\nsWJFnT17VllZWapYsaJ69epVnLUCAAAAAACgAFbfQn3p0qWqWbOmkpKStHDhQklSfHy8du/erYkT\nJyo9PV1hYWHFVigAAAAAAAAKZnXI88cff6hjx47y8vJSlSpV5OPjo127dsnZ2VlPP/20QkND9d57\n7xVnrQAAAAAAACiA1SGPm5ubPD09zY+rVKmilJQU8+OGDRsqKSnJttUBAAAAAADAKlaHPPfff792\n7NhhfnzfffdZbLJ85swZmUwm21YHAAAAAAAAq1gd8vTs2VPr1q1T3759lZGRobZt22r//v0aN26c\nEhISNH/+fAUFBRVnrQAAAAAAACiA1XfXat++vTIzM/XRRx+pVKlSeuSRR9StWzctXrxYkuTn56fR\no0cXuZDNmzdr+PDh2rt3r7ntxx9/VNeuXfP17du3r0aMGFHksQAAAAAAAByN1SGPJHXv3l3du3c3\nP54wYYIiIiKUnp6u+++/X25ubkUqYu/evRo+fHi+9uTkZHl4eCghIcGi/e677y7SOAAAAAAAAI6q\nUCGPJGVmZmrXrl1KTU1V8+bN5e3tLWdn5yIFPNnZ2UpISND06dPl6empnJwci+MpKSmqUaOG6tSp\nU+hzAwAAAAAA3E4KFfIsWrRIU6dOVWZmpgwGgypXrqyLFy9q4MCB6tWrl0aMGCGDwWD1+b7++mvF\nxcVp5MiROnv2rObNm2dxPCUlRdWrVy9MiQAAAAAAALclqzdeXrdunSZMmKBHHnlEU6ZMMd9JKzAw\nUK1atVJ8fLwWLFhQqMGDgoK0ZcsWhYeHX/X4r7/+qpMnT6pTp06qXbu2WrdurZUrVxZqDAAAAAAA\ngNuB1SHP7Nmz1aRJE73//vt6+OGHze1+fn6aPn26QkNDzZswW8vX11fe3t5XPfbXX3/p3LlzOnbs\nmF588UXJ9TowAAAgAElEQVTFxcWpYcOGGjVqFEEPAAAAAADAf1h9udbvv/+ukSNHFnj80Ucf1Tvv\nvGOToiTpzjvvVHx8vKpXr65y5cpJkho3bqxTp07pgw8+UKdOnWw2FgAAAAAAwK3O6pDH29tbZ8+e\nLfD4sWPHClyVUxTu7u5q3LhxvvamTZtq27ZtysrKkoeHR6HOabxstFV5JUJOdrYyMzPtXQZuU7m5\nuZLEexCwEeYUYHvMK8D2mFeAbeXNKVux+nKtli1basGCBTp69Gi+zZV37typhQsX6tFHH7VZYYcP\nH9bChQuVnZ1t0X7p0iWVKlWq0AEPAAAAAACAI7N6Jc+QIUO0a9cudezYUTVr1pQkxcXFKTo6Wvv2\n7ZOfn58GDx5ss8JSU1M1ceJE3X333WrVqpUkyWQyaePGjWrQoEGRzuns4myz+koCVzc3eXl52bsM\n3KbyPr3hPQjYBnMKsD3mFWB7zCvAtmy9Ks7qkKds2bJaunSp5syZo82bN8vd3V27du1SxYoV1atX\nL73wwgsqW7aszQpr1KiRgoODNW7cOKWnp6t8+fL67LPPdPDgQS1atMhm4wAAAAAAADiCAkOeZ599\nVv369VOzZs0kSbt27VLVqlU1ePBgm67YyWMwGCwuA3NyclJMTIyioqI0ffp0nTt3TrVq1dK8efPM\nK4kAAAAAAABwRYEhz759+3Ty5Enz42effVZTpkxR+/bti6WQQYMGadCgQRZtd955pyZOnFgs4wEA\nAAAAADiSAkOeSpUqKTY2VqmpqfL09JQkbd26Vampqdc8YUREhG0rBAAAAAAAwHUVGPKMGTNGw4cP\nV2xsrLltzZo1WrNmzTVPSMgDAAAAAABw8xUY8jRt2lTbt2/X6dOnlZOTo1atWmn06NFq2bLlzawP\nAAAAAAAAVrjm3bWcnJzk6+srSRo4cKAeeugh+fv735TCAAAAAAAAYD2rb6H+8ssvF2cdAAAAAAAA\nuAFO9i4AAAAAAAAAN46QBwAAAAAAwAEQ8gAAAAAAADiAAkOexx57TGvXrjU/XrlypY4fP35TigIA\nAAAAAEDhFBjy/PXXX/r777/Nj0eNGqV9+/bdlKIAAAAAAABQOAXeXeu+++7TjBkzdODAAXl6ekqS\nli9frj179lzzhOPHj7dpgQAAAAAAALi+AkOet956S2PHjtW6det0+fJlSVJSUpKSkpKueUJCHgAA\nAAAAgJuvwJCnVq1aWr58uflxYGCgIiMj1aFDh5tSGAAAAAAAAKxn9d21Jk2apODg4OKsBQAAAAAA\nAEVU4Eqe/woLC5PRaNTSpUu1efNmpaamytXVVb6+vmrWrJnCwsLk5MQd2QEAAAAAAOzB6pDn4sWL\nioiI0K5du+Tt7a2AgABdvHhR27dvV2JiopYtW6aEhAS5ubkVZ70AAAAAAAC4CqtDnpkzZ2r37t0a\nNWqUevbsKVdXV0lSdna2Fi5cqMmTJ+vDDz/U4MGDi61YAAAAAAAAXJ3V11etXbtWXbp0Ue/evc0B\njyS5ubmpd+/e6tKli9asWVMsRQIAAAAAAODarA55Tp06pVq1ahV4vGbNmkpNTbVJUQAAAAAAACgc\nq0MePz8/7d27t8Dje/fula+vr02KAgAAAAAAQOFYHfKEhYXpiy++UHR0tDIyMsztGRkZev/997V6\n9Wp17NixWIoEAAAAAADAtVm98XJERIR++uknxcTEaNasWSpXrpxMJpPOnDkjk8mk5s2ba8CAAcVZ\nKwAAAAAAAApgdcjj4uKimTNn6quvvtKWLVv0xx9/yGQyqVKlSgoNDVXz5s2LsUwAAAAAAABci9Uh\nT55mzZqpWbNmxVELAAAAAAAAisjqPXkAAAAAAABQchHyAAAAAAAAOABCHgAAAAAAAAdQYkKezZs3\nKyQkJF97TEyMmjdvrnr16qlv3746dOiQHaoDAAAAAAAo2awOeXr27Klly5YVSxF79+7V8OHD87XP\nnDlTsbGx6tevn6KionT+/Hn17t1bGRkZxVIHAAAAAADArcrqkOeHH37Q5cuXbTp4dna24uLi1KtX\nL7m6ulocy8jI0Ny5c/Xyyy8rPDxcoaGhmjt3rjIzM7V06VKb1gEAAAAAAHCrszrkadiwob7++mvl\n5ubabPCvv/5acXFxGjlypMLDw2UymczH9u/fr6ysLIWGhprbfHx81LBhQ23bts1mNQAAAAAAADgC\nF2s7hoSEaO7cuWrWrJnq1aunMmXKyMkpf0Y0fvx4qwcPCgrSli1b5O3trRkzZlgcO3LkiCTpnnvu\nsWj39/fXli1brB4DAAAAAADgdmB1yDNz5kxJUlZWlhITEwvsV5iQx9fXt8BjGRkZcnNzk4uLZYle\nXl7KzMy0egwAAAAAAIDbgdUhT3JycnHWkY/JZJLBYLjqsYLar8d42XgjJZU4OdnZBF6wm7xLN3kP\nArbBnAJsj3kF2B7zCrAtW26JIxXxFuqZmZk6dOiQLly4YPPNmPOULl1a2dnZMhotg5nMzEz5+PgU\ny5gAAAAAAAC3KqtX8kjSTz/9pHfffVd79uyRyWTSvHnzJF25RGvkyJEWmyTfqMqVK8tkMunEiROq\nXLmyuf3EiRO69957i3ROZxdnW5VXIri6ucnLy8veZeA2lffpDe9BwDaYU4DtMa8A22NeAbZl61Vx\nVq/k+fnnnxUeHq4///xT3bp1M98Jy9vbW5cvX9bLL7+sb775xmaFBQcHy93d3WL/n/T0dO3cuVON\nGze22TgAAAAAAACOwOqVPNOmTZOvr6+WL1+uixcvatGiRZKu3CFr1apV6tmzp2JiYtS0aVObFObl\n5aXw8HBFR0fLyclJlStXVmxsrHx8fNS1a1ebjAEAAAAAAOAorA559u7dq4EDB8rT01MXL160OObt\n7a2uXbsqOjq6yIUYDIZ8GyoPHTpUTk5OmjdvnjIzMxUSEqLIyEh5e3sXeRwAAAAAAABHZHXI4+Tk\nlO925v+WlZVlvoSrKAYNGqRBgwZZtDk7O2vYsGEaNmxYkc8LAAAAAABwO7B6T5769etrxYoVysnJ\nyXfs7Nmz+vTTTxUcHGzT4gAAAAAAAGAdq1fyDB06VM8884zCwsL06KOPSpK+/vprJSUlacmSJcrI\nyND7779fbIUCAAAAAACgYFav5AkMDNSCBQtUunRpzZ07V5IUHx+v2bNnq0KFCpo3b57q1KlTbIUC\nAAAAAACgYFav5JGkmjVrauHChUpLS9OJEydkNBpVsWJF+fr6Fld9AAAAAAAAsILVK3n+7dSpUzp9\n+rTS09OVkZFh65oAAAAAAABQSIVaybNmzRpNnTpVJ0+etGi/9957NXbsWDVp0sSmxQEAAAAAAMA6\nVoc869at07Bhw1S1alWNGjVKAQEBMplMOnLkiBYtWqT+/ftr7ty5atSoUXHWCwAAAAAAgKuwOuSZ\nNWuWgoKCtGDBArm5uVkc69Gjh7p3766oqCgtXrzY5kUCAAAAAADg2qzek+fQoUPq1KlTvoBHkjw9\nPdWlSxf98ssvNi0OAAAAAAAA1rE65PH399fhw4cLPH7u3Dn5+fnZpCgAAAAAAAAUjtUhz7Bhw7R4\n8WItWrRIubm5Fsc2bdqkhIQEvfrqqzYvEAAAAAAAANdX4J48oaGhMhgMMplMFv87YcIEvf/++woI\nCJAknTx5UmfOnNEdd9yhBQsWqG3btjeteAAAAAAAAFxRYMjz4IMPWnWCatWqmf+/wWC48YoAAAAA\nAABQaAWGPO++++7NrAMAAAAAAAA3wOpbqOfJycnRmTNn8u3Lk6dixYo3XBQAAAAAAAAKx+qQ5/jx\n43r99de1e/dumUymq/YxGAzcRh0AAAAAAMAOrA553njjDe3bt09dunRRpUqV5OzsXJx1AQAAAAAA\noBCsDnn279+vF154QYMGDSrOegAAAAAAAFAETtZ2LFeunLy9vYuzFgAAAAAAABSR1SFP//79NX/+\nfB06dKg46wEAAAAAAEARWH25VlhYmNavX68OHTqocuXKKlu2rAwGQ75+H330kU0LBAAAAAAAwPVZ\nHfJMmTJF27dvl7u7u7Kzs3X69Ol8fa4W+gAAAAAAAKD4WR3yrFixQs2bN9d7770nDw+P4qwJAAAA\nAAAAhWT1njxGo1GhoaEEPAAAAAAAACWQ1SFPixYt9OWXXxZnLQAAAAAAACgiqy/Xevrpp/Xaa6+p\nV69eatGihcqVKydnZ+d8/dq2bWvTAgEAAAAAAByRrfc2tjrkefbZZyVJf/31l7777rur9jEYDDYP\nec6ePavGjRvna2/Tpo2io6NtOhYAAAAAACi5Fm1M1rnzl+xdhs34eLoq8G7bnc/qkCchIcF2oxZC\ncnKyJCk+Pl5eXl7m9jvvvNMu9QAAAAAAAPs4d/6STp/LsncZNmM0GiV7hDyNGjWy3aiFkJKSovLl\ny191NQ8AAAAAAACusDrkWbt2rVX9bH25VkpKimrUqGHTcwIAAAAAADgaq0OeoUOHWtWvOEKeUqVK\nqXv37vr5559VpkwZPffcc3r++edtOg4AAAAAAMCt7Ib25MnNzdWZM2e0YcMG/frrr4qJibFpcUaj\nUYcOHZKXl5eGDx+uSpUq6csvv9S0adN08eJFDRw40KbjAQAAAAAA3KpssifPk08+qQEDBig2NlaR\nkZE2KUy6creuuLg4+fn5yd/fX5LUsGFDXbhwQXPmzFFERITc3NxsNh4AAAAAAMCtyuqQ53pCQ0Nt\nGvBIkpOTkxo2bJivvWnTpvr000917NgxVatWzerzGS8bbVme3eVkZyszM9PeZeA2lZubK0m8BwEb\nYU4Btse8AmyPeQV7MhgMMhqNDvW3vcnG53Oy1YmSk5NlMBhsdTpJ0qlTp7R48WKlpaVZtF+6dEmS\nVKZMGZuOBwAAAAAAcKuyeiXP7NmzrxriZGdnKzk5WYmJierQoYNNi7t06ZLGjRunrKws9e7d29y+\nYcMG3XvvvSpXrlyhzufs4mzT+uzN1c1NXl5e9i4Dt6m8T294DwK2wZwCbI95Bdge8wr25uzs7FB/\n29t2qUwhQp6oqKiCT+LiotatW2v06NE2KSpPQECA2rZtq+joaDk5Oalq1apav369EhMT9eGHH9p0\nLAAAAAAAgFuZ1SHPpk2brtru7OysO++8Ux4eHjYr6t8mTZqkDz74QAkJCTp9+rSqVaumGTNmqEWL\nFsUyHgAAAAAAwK3I6pAn7+5WN1upUqU0bNgwDRs2zC7jAwAAAAAA3AoKDHnWrl1bpBO2bdu2yMUA\nAAAAAACgaAoMeYYOHVrokxkMBkIeAAAAAAAAOygw5ElISLjuk3Nzc5WQkKCtW7dKktq0aWOzwgAA\nAAAAAGC9AkOeRo0aXfOJu3fv1ltvvaWDBw+qSpUqeuONN9SkSRObFwgAAAAAAIDrs3rj5TxpaWmK\njIzUypUrVapUKb366qvq16+fXF1di6M+AAAAAAAAWMHqkMdkMmnRokV6//339c8//6hFixYaM2aM\nKlWqVJz1AQAAAAAAwApWhTwHDhzQ+PHj9dNPP6lSpUqaPHmyWrRoUdy1AQAAAAAAwErXDHn++ecf\nTZs2TUuWLJGzs7MGDBigF198Ue7u7jerPgAAAAAAAFihwJBn+fLlmjp1qtLS0vTwww9r7NixqlKl\nyk0sDQAAAAAAANYqMOR5/fXXzf9/165d6tixo6Qre/P8l8FgkMlkksFg0P79+4uhTAAAAAAAAFxL\ngSFPp06dCn0yg8FwQ8UAuHXkhbsAAAAAgJKhwJDn3XffvZl1AA5t0cZknTt/yd5l2JSPp6s6N6ti\n7zIAAAAAAP+f1bdQB1B0585f0ulzWfYuw6aMRqO9SwAAAAAA/IuTvQsAAAAAAADAjSPkAQAAAAAA\ncACEPAAAAAAAAA6AkAcAAAAAAMABEPIAAAAAAAA4AEIeAAAAAAAAB0DIAwAAAAAA4AAIeQAAAAAA\nABwAIQ8AAAAAAIADIOQBAAAAAABwAIQ8AAAAAAAADoCQBwAAAAAAwAEQ8gAAAAAAADgAQh4AAEoA\ng8Fg7xIAAABwi3OxdwHW+OyzzzRnzhz99ddfeuCBBzRq1CjVq1fP3mWhmCzamKxz5y/ZuwybuadC\naXuXADgkR/u3wsfTVZ2bVbF3GQAA3JYc7feKO0u765nWgfYuA3ZQ4kOeFStWaPz48Ro4cKCCgoL0\n8ccf6/nnn9eqVavk7+9v7/JQDM6dv6TT57LsXYbNlCntbu8SAIfkaP9WGI1Ge5cAAMBty9F+r8Dt\nq0RfrmUymTRjxgx169ZNAwcO1KOPPqqYmBiVKVNG8+fPt3d5AAAAAAAAJUaJDnmOHj2qP//8U6Gh\noeY2FxcXNW/eXNu2bbNjZQAAAAAAACVLiQ55jhw5IkmqXLmyRbu/v7+OHz8uk8lkh6oAAAAAAABK\nnhId8mRkZEiSvLy8LNq9vLyUm5urCxcu2KMsAAAAAACAEqdEhzx5K3UKuq2sk1OJLh8AAAAAAOCm\nKdF31ypd+sqtpzMzM1W2bFlze2ZmppydneXh4VGo87VuFGDT+uzN083gcKuZDAaDfDxdHeouM94e\nrjLm5jrU1yRduS2jJId7D+LW4Ij/VjCngOLDvAJsz5HmlSP+XuHj6aqsrCyH2+LEEb9XV34HvGiz\n85XokCdvL57jx48rIOD/Aprjx4/r3nvvLfT5XC+l2qy2kuDYYcf6evIE3i3pbntXYUvn5Ocp6S57\n12FrF/XLL7/Yuwjcxhzv3wrmFAAA9uJ4v1dk6eeff7Z3EcXC8b5Xtgt4pBIe8lSpUkV+fn5KTExU\nkyZNJEk5OTnaunWrWrRoUahz1a9fvzhKBAAAAAAAKBFKdMhjMBgUERGhN998Uz4+PgoJCdEnn3yi\n9PR09e7d297lAQAAAAAAlBgG0y1wkV58fLw++ugjnT17Vg888IBGjRqlunXr2rssAAAAAACAEuOW\nCHkAAAAAAABwbdyDHAAAAAAAwAEQ8gAAAAAAADgAQh4AAAAAAAAHQMgDAAAAAADgAAh5AAAAAAAA\nHAAhDwAAAAAAgANw+JDns88+U+vWrVW3bl11795d+/bts3dJwC0jNzdX8fHxeuKJJxQcHKx27dpp\nwYIFFn1iYmLUvHlz1atXT3379tWhQ4fsVC1w68nOztYTTzyh0aNHW7Qzr4DCS0pK0lNPPaW6desq\nNDRUM2bMUG5urvk48wqwnslk0vz589WmTRsFBwfr6aef1rfffmvRhzkFWGfz5s0KCQnJ1369OZSd\nna1JkyapadOmCgkJ0SuvvKJTp05ddzyHDnlWrFih8ePHq2PHjpoxY4ZKly6t559/XidOnLB3acAt\n4YMPPtB7772nTp06KSYmRk888YQmTZqkOXPmSJJmzpyp2NhY9evXT1FRUTp//rx69+6tjIwMO1cO\n3Bpmzpypw4cP52tjXgGFs2fPHkVERKhatWqaPXu2evbsqbi4OH344YeSmFdAYSUkJGjKlCnq0qWL\nPvzwQwUEBKhfv3765ZdfJDGnAGvt3btXw4cPz9duzRwaN26cVq1apddee03vvPOOUlJS1L9/f4sP\nMK7K5KByc3NNLVq0MI0fP97clpOTY2rZsqXpzTfftGNlwK3h8uXLppCQEFN0dLRF+4QJE0yNGzc2\nZWRkmOrVq2eKi4szH0tPTzeFhISY4uPjb3K1wK3np59+MtWrV8/00EMPmUaNGmUymUym8+fPM6+A\nInjmmWdML7zwgkXb1KlTTc8++yw/r4AiePLJJ00jR440PzYajabmzZubJk6cyM8qwAqXLl0yzZ49\n21S7dm3Tgw8+aAoODjYfs2YOHT161PTAAw+Y1q5da+5z5MgRU2BgoGnjxo3XHNthV/IcPXpUf/75\np0JDQ81tLi4uat68ubZt22bHyoBbQ2Zmpjp37qzWrVtbtFepUkVpaWn69ttvlZWVZTHHfHx81LBh\nQ+YYcB2XL1/W66+/rn79+snX19fcvn//fuYVUEhpaWn6/vvv1a1bN4v2YcOG6aOPPtK+ffuYV0Ah\nZWRkyMvLy/zYyclJ3t7eSk9P52cVYIWvv/5acXFxGjlypMLDw2UymczHrJlDeZdHtmjRwtyncuXK\nqlat2nXnmcOGPEeOHJF05YX4N39/fx0/ftziRQaQn4+Pj8aMGaPAwECL9i+//FJ+fn5KTU2VJN1z\nzz0Wx/39/fNdfgLAUlxcnIxGo/r372/x8yjvZxfzCrBeSkqKTCaTSpUqpQEDBqhOnTpq0qSJZs6c\nKZPJxLwCiqBDhw5atWqVkpKSdP78eSUkJOi3335Tu3btmFOAFYKCgrRlyxaFh4fnO2bNHDp8+LDu\nuusulSpVyqJPQEDAdeeZyw3UXaLlXcv27wQ673Fubq4uXLiQ7xiAa1uyZImSkpI0duxYZWRkyM3N\nTS4ulv+MeHl5KTMz004VAiXf77//rlmzZikhIUGurq4Wx5hXQOGdPXtWkjRy5Ei1b99effv21c6d\nOxUTEyN3d3fl5uYyr4BCeuWVV5SSkqI+ffqY24YMGaIWLVpo1qxZzCngOv69Uvu/rPl9LzMzU56e\nnvme6+npaf6wvSAOG/LkfTJqMBiuetzJyWEXMQHF4vPPP9e4ceP0+OOPq2fPnoqNjS1wfhXUDtzu\ncnNz9b///U9du3ZV3bp1JVnOF5PJxLwCCiknJ0eS9Mgjj5g3t3zwwQd19uxZxcTEqH///swroJCG\nDx+u77//XuPHj9d9992n7du3a8aMGfL29uZnFXCDrjWH8nIKa/oUxGFDntKlS0u6koCVLVvW3J6Z\nmSlnZ2d5eHjYqzTglhMfH6/IyEi1bNlSU6dOlXRljmVnZ8toNMrZ2dncNzMzUz4+PvYqFSjRPv74\nY6WmpiouLk6XL1+WdOWHuMlk0uXLl5lXQBHkrcx+5JFHLNobN26sBQsWMK+AQjpw4IDWrl2r6Oho\ntWnTRpLUsGFDGY1GTZ06VUOGDGFOATfgWj+X8nIMb2/vq66M+3efgjjscpa8vXiOHz9u0X78+HHd\ne++99igJuCVFRUVp8uTJ6tSpk6ZPn25eVli5cmWZTCadOHHCov+JEyeYY0ABNm3apNTUVDVs2FC1\na9dW7dq1lZKSopUrV6p27dpydXVlXgGFlLenQd6Knjx5QSrzCiico0ePSpLq1atn0R4SEqKsrCwZ\nDAbmFHADrPk7qkqVKvr777+VnZ1dYJ+COGzIU6VKFfn5+SkxMdHclpOTo61bt+qhhx6yY2XArSMh\nIUGzZ89Wr1699M4771gsDQwODpa7u7vFHEtPT9fOnTvVuHFje5QLlHgTJ07UsmXLzP8tXbpUVapU\nUYsWLbRs2TK1bduWeQUU0v333y9fX1+tW7fOov2rr76Sr68v8woopICAAEnSnj17LNr3798vFxcX\ntW7dmjkF3ABr/o5q3LixjEajNm/ebO5z5MgR/fbbb9edZw57uZbBYFBERITefPNN+fj4KCQkRJ98\n8onS09PVu3dve5cHlHinTp3S1KlTVb16dbVt21b79u2zOB4UFKTw8HBFR0fLyclJlStXVmxsrHx8\nfNS1a1c7VQ2UbFf75MXd3V133nmnatWqJUnMK6CQDAaDhgwZolGjRmn8+PFq06aNduzYoZUrV2rC\nhAny9vZmXgGFULduXTVp0kQTJkzQuXPnVLVqVe3cuVNz5szRc889J19fX+YUcAO8vLyuO4fuuece\nPf744+Yb3pQuXVpRUVEKDAxUq1atrnl+hw15JKlHjx66dOmSPvroIyUkJOiBBx7Q3Llz5e/vb+/S\ngBLvm2++UU5Ojg4ePKhu3bpZHDMYDEpKStLQoUPl5OSkefPmKTMzUyEhIYqMjJS3t7edqgZuPf/d\nVI95BRRep06d5OrqqtjYWC1fvlx+fn6aOHGinnrqKUnMK6CwYmJiFBMTo4SEBJ06dUr33HOPxo4d\na/6dkDkFWM9gMPy/9u4/Kucz/uP4M4QhhMSmOSN1DyU/RkLKUkvpbH7lrHEMy/yacObXwdiasJYf\ny0nYUSKq0bFq7k5+zZiZsWN2LI6sETKEozES3z863V93EZH5fm+vxzn++Fyfz+e6rs91/8Pb9X5f\nT/T3vYiICCIiIoiMjOTu3bt4eHgwe/bsRxY4t7pXegyViIiIiIiIiIj8v2WxNXlERERERERERF4k\nCvKIiIiIiIiIiFgABXlERERERERERCyAgjwiIiIiIiIiIhZAQR4REREREREREQugII+IiIiIiIiI\niAVQkEdERERERERExAIoyCMiImJBCgsLGTt2LG5ubgwePJgTJ06Ue2bKlCnMnDnzOcwOZsyYgaur\n60OvnzWDwcAnn3zyn433OIYNG4a/v3+V9JWXl0dISAiurq50796d69evV0m/97t+/TrXrl2r8n5F\nRETk6SnIIyIiYkFiYmI4fPgwU6ZM4aWXXmLChAkUFxeb7ufk5JCVlcXYsWOf2xytrKwqvP6vx3/e\nxo4dy/Tp06ukr0WLFnHkyBEmTJjA1KlTsbGxqZJ+S/3+++/4+/vz119/VWm/IiIiUjVqPO8JiIiI\nSNUxGo0MHTqU4cOH4+Xlha+vL0eOHKFTp05ASRAoICCAV1999bnN8d69exVev2g8PDyqrK8TJ07g\n6upKaGholfVZtv9Lly49k75FRETk6SnIIyIiYkEuXrxIixYtAGjevDkAFy5cACA3N5fMzEzS09Of\n2/zk2SoqKqJOnTrPfJwXPTAnIiLyf5XStURERCyIra2tqQ5Lad2Uhg0bAiW7ePz9/WnZsuVj93fg\nwAEMBgMHDhxg5syZdOvWjY4dOzJ+/HjOnj1reu6rr77CYDBw+fJls/eftuZOTEwMBoOhXHpQYWEh\nLi4uLFy4ECgJOiQkJDBgwAA6duxIhw4dCAoKYsuWLRX2/6AaPXl5eRgMBlatWmXWvnHjRgIDA3Fx\nccHT05OIiAhu3Lhh9sz69esJCAigQ4cOeHh48PHHH5Ofn1/hHMrW5OnTpw8LFiwgKSkJPz8/XF1d\n6QYGcV4AAAtJSURBVN+/P0aj8aF9lP5O586dY+/evRgMBqKjowG4c+cOMTEx9O3bFxcXF3x8fFix\nYoVZGh/Ab7/9xtixY3F3d6d9+/Z4enoyd+5cCgsLgZLfeNasWQAEBwczfPjwB87//u8YPXq02fWn\nn37K1KlTcXFxwc/Pj6KiIgAyMzMZMGAAHTp0oHv37syaNYuCgoIK101ERETK004eERERC9KlSxdS\nU1Px8vLim2++wcbGhvbt23PmzBm+++47vv322yfqd8aMGTg4OBAWFkZeXh5xcXFcunSJpKSkR777\nNDVwAgMDWbZsGUajkTFjxpjad+zYQVFREQEBAQBERUWxZs0ahgwZwrvvvsvVq1dJSUlh1qxZNG/e\nnO7du1d6fve3R0VFsXr1avr378+wYcM4deoUiYmJHD16lISEBKpXr87WrVsJDw9n4MCBvP/++5w/\nf564uDiOHTtGenp6hetQ9l5WVhbp6ekMHz6cevXqER8fz+TJk2nTpg2tW7cu976joyOLFy8mIiIC\ne3t7Ro0ahbOzMwDTp08nMzOTIUOG4OzszNGjR4mOjiYnJ4eoqCgAsrOzCQkJwWAwMGHCBKytrdm3\nbx/JycncvHmTL774Al9fXy5evEhycjITJ040pQA+7hoCpKam0rZtW+bMmcONGzewtrZm06ZNzJs3\nD29vbwYPHkx+fj4bNmzg0KFDbN68mXr16j103URERMScgjwiIiIWJCwsjBEjRtCvXz9q1qxJeHg4\nNjY2RERE4Ovry2uvvfZE/bZo0YJ169aZrv/55x82bdpEfn4+zZo1q/Ddp0ntcXBwwNXVlczMTLMg\nj9FopGXLlri4uFBUVERiYiKDBg1i/vz5pmd8fHx466232LdvX4VBnkfJzc1l9erVfPTRR2YFqz08\nPBgzZgxbt25lwIABpKen4+TkxOeff256plmzZiQmJnLhwoVHrtP9Ll68SEZGhmnXlaurK0OGDMFo\nNDJ+/Phyzzdu3JigoCCWLl2KnZ0d/fv3B2D//v1kZGSwePFigoKCgJJdOK+//jqfffYZwcHBdOvW\njY0bN1KnTh3i4+NN6V7BwcEMHTqUffv2AeDs7IybmxvJycn06tXriXZoFRcXExMTQ/369YGSk7oW\nLVrEoEGDCA8PNz3n7+/PwIEDWbt2LRMnTqz0OCIiIi8qpWuJiIhYEAcHB7Zt28bmzZvZs2cPQUFB\n5OXlkZ6ezrhx4yguLmbx4sX07t2bwMBAsrKyHqtfX19fs2uDwQDwnxThDQgI4NixY5w5cwYoSdXa\nu3cv/fr1A8Da2pr9+/ebHQt/7949/v33X4ByKVWVtXPnTu7du4eXlxcFBQWmPy4uLjRo0IDdu3cD\nJTWQcnJyWLlypSlFa/DgwaSmplYqwAPg5ORkllZXut6VTWHavn07NWrUwMPDw2zuvXv3xsrKyjT3\nefPmkZmZaVbPp6CggDp16nDz5s1KjVkRR0dHU4AH4Mcff+TmzZt4e3ubza9p06Y4Ojqa5iciIiKP\nRzt5RERELEzNmjVp166d6To2NhYfHx9at27Nhg0b2Lx5M1FRUeTm5hIWFkZaWhqtWrWqsM9GjRqV\nGwPg7t27Vf8BZfTr149FixaRmZnJ6NGjy6VqAdSoUYPt27ezc+dOTp06RW5urim487RzPH36NADv\nvPPOA++XFrYeN24chw4dYunSpSxdupS2bdvi4+PD4MGDsbOzq9SYtra2Ztel6122js7jzP3OnTv0\n7Nmz3D0rKyvT3K2srLh06RIrVqwgOzub3NxcLl68CECtWrUqNWZFyn5X6do+aHcSQJMmTapsbBER\nkReBgjwiIiIW7Pz582zdutVUgNhoNOLn50ePHj3o0aMHCQkJGI1Gxo0bV2E/T1pXp7JBiQexs7Oj\na9euGI1GRo8ejdFoxMnJCUdHR6Bk186YMWPYu3cvXbt2xd3dnREjRvDGG2/g7e1d6fHKBoVKr9es\nWUP16tXLPV+3bl2gJDUrLS2Nffv2sWPHDvbs2cPy5cuJi4sjJSWlUgWvq1Wrms3Wd+/exdbW1lR7\np6zGjRsDkJaWxrRp03BwcKBr16707dsXNzc31q9fX2HB54o86Lcv+12la7to0SKaNm1a7nlra+sn\nGltERORFpSCPiIiIBYuNjaVPnz6mgMjly5fp3Lmz6X7Dhg1NOzaeRuk/3m/fvm3WXlBQ8FSFl0sF\nBAQwZ84c/vzzT/bu3WtWp+XgwYP88MMPTJkyhdDQUFP743xXtWrVys257AlhpUfRv/LKK+VqGmVl\nZZl2m+Tk5HD37l169epFr169gJJToyZNmsSWLVuYPHlyJb64ajRv3pyffvqJTp06me3IKSoqYseO\nHbRo0QKAJUuW4OzsTHJysmnXEJSsxaN+vwetYXFxMVevXn2s+UFJsKls3aQ9e/ao6LKIiEglqSaP\niIiIhbpw4QKpqalmqTB2dnbk5eUBJbsozp49i729/VOPVZqO9Mcff5iNf/jw4Ue++zhBID8/P6yt\nrVmwYAFFRUWmejyAKZhQNuVs/fr1QMW7iZo0aUJ2drZZW9mdK6W7gcoeqb5r1y4mTpzI9u3bgZKi\n19OmTTPbCeTi4gKUpJM9D97e3hQXF7N69Wqz9qSkJMLCwvj1118BuHbtGi1atDAL8Bw/fpyDBw+a\nrV9pMO/+tiZNmpCfn8+VK1dMbbt37+bWrVuPnF/Pnj2xtrbm66+/Nlu37OxsxowZ81int4mIiMj/\n0k4eERERC7Vq1Sp69+5NmzZtTG2+vr4sXLiQ1q1bc+bMGa5cuYKfn99Tj+Xj40N4eDhz587l9OnT\nFBcXk5iYSNOmTTl//nyF7z7O6Vv169enZ8+e7Nq1Czc3N9MOFIDOnTtTt25dwsPDOXPmDLVr1+b7\n77/n6NGj2NraUlhY+NB+AwICWLt2LVOmTMHd3Z1Dhw5x8OBBszQhg8FAcHAwSUlJFBQU4OnpyYUL\nF0hISKBly5aEhIQAMHLkSGbOnMmoUaPo27cvt2/fJiUlhbp16/L2228/9Ro8iTfffBNPT0+io6PJ\nzc2lS5cunDx5kk2bNtGxY0f8/f0B8PT0xGg0Eh4ejrOzM6dOnSIlJYVXX32VkydPcuvWLWrVqmVK\n79qwYQNXrlyhT58+9O/fn4yMDD744AMGDRrEuXPnSEpK4uWXX37kdzVq1IiJEycSFRXFe++9h7+/\nP9evX2f9+vXY2try4YcfPpN1ERERsVQK8oiIiFigv//+m82bN7Np0yaz9uDgYE6fPk18fDwNGjQg\nKirqkceqP2ynzf3tjRo1IjY2li+//JKoqCjs7e0ZOXIkt27dYtmyZWbv3P9e2euKBAYGsnv3brNd\nPFCS6hMbG0tkZCTLly+ndu3aeHt7k5qaysKFC/nll18e2uekSZMoKioiIyODXbt20a1bN9atW2c6\ngrzU/PnzadWqFcnJySxcuJBGjRrh7+9PWFiY6bSo0sLMCQkJREZGUq1aNTp37kxkZCQODg4VfltV\npLQ9THR0NCtXriQtLY3MzEyaNm1KSEgIEyZMMAWz5s2bR+3atdm2bRspKSm0a9eOJUuWABAaGsrP\nP/9Mr169cHd3x9fXl6ysLI4fP06fPn3w8vJi9uzZxMfHs2DBApycnFi2bBlxcXGPVZMpNDQUe3t7\n4uPjiYyMxMbGhi5dujB58mSzYJ6IiIg8mtW9Z/VfRyIiIiIiIiIi8p9RTR4REREREREREQugII+I\niIiIiIiIiAVQkEdERERERERExAIoyCMiIiIiIiIiYgEU5BERERERERERsQAK8oiIiIiIiIiIWAAF\neURERERERERELICCPCIiIiIiIiIiFkBBHhERERERERERC6Agj4iIiIiIiIiIBfgfR6E8b7GxAqYA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105e2b610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "percentnulls = {}\n",
    "for i in range(len(df.columns)):\n",
    "    percentnulls[df.columns[i]] = 100.0 * df[df.columns[i]].isnull().sum() / len(df[df.columns[i]])\n",
    "    \n",
    "# plt.barh(range(len(df.columns)),percentnulls.values(), align=\"center\")\n",
    "# plt.ylim([0,20])\n",
    "# plt.yticks(range(len(df.columns)), percentnulls.keys())\n",
    "# plt.ylabel(\"feature\")\n",
    "# plt.xlabel(\"% nulls\")\n",
    "# plt.title(\"% Nulls Per Feature\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(range(len(df.columns)),percentnulls.values(),s=100)\n",
    "# plt.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n",
    "# plt.ylabel(\"% null values\")\n",
    "# plt.title(\"Proportion of Null Values By Feature\")\n",
    "# plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize = (16,4))\n",
    "plt.hist(percentnulls.values(),bins=20,alpha=0.7,edgecolor=\"white\")\n",
    "plt.xlabel(\"% null values in feature\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.title(\"% Null Values Per Feature\")\n",
    "plt.gca().xaxis.grid(True,color='0.95')\n",
    "plt.gca().yaxis.grid(True,color='0.95')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['addInfo']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in percentnulls.keys() if percentnulls[i] > 20 and percentnulls[i] < 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows us that (apart from the `addInfo` feature), if a feature has $>50%$ missing values, it has $>80%$ null values, and if there are $<50%$ nulls, there are $<20%$ null values. This means if we remove columns with $>50%$ missing data, we will be removing most of the nulls in the data frame, and it won't actually remove much information. As for the `addInfo` feature, this is additional information about the candidate, and at this stage we are not using it in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus remove any column that has 50% or more null values. This takes out any predictors that would have been useless anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop:  ['classrank', 'canAfford', 'firstinfamily', 'artist', 'workexp', 'visited', 'acceptProb']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = []\n",
    "for i in df.columns:\n",
    "    if 1.0* df[i].isnull().sum() / len(df[i]) >= 0.5:\n",
    "        cols_to_drop.append(i)\n",
    "print \"Columns to drop: \", cols_to_drop\n",
    "dfr = df.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop all columns where the accept status is NaN. These are of course useless as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfr = dfr[pd.notnull(df[\"acceptStatus\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction nulls =  0.0315301778112\n"
     ]
    }
   ],
   "source": [
    "x = dfr.isnull().sum(axis=1).tolist()\n",
    "y = float(sum(x)) / (dfr.shape[0]*dfr.shape[1])\n",
    "print 'Fraction nulls = ', y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have only 3% null values. Good! The next step is to choose which columns we want to use to predict. Obviously columns like `studentID`, while crucial, are not actually predictors. Also, we remove weighted GPA in favour of GPA, as we have already normalised everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'studentID', u'admissionstest', u'AP', u'averageAP', u'SATsubject',\n",
      "       u'GPA', u'GPA_w', u'program', u'schooltype', u'intendedgradyear',\n",
      "       u'addInfo', u'female', u'MinorityGender', u'MinorityRace',\n",
      "       u'international', u'sports', u'collegeID', u'earlyAppl', u'alumni',\n",
      "       u'outofstate', u'acceptStatus', u'name', u'acceptrate', u'size',\n",
      "       u'public', u'finAidPct', u'instatePct'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print dfr.columns\n",
    "predictor_cols = [\"admissionstest\",\"AP\",\"averageAP\",\"SATsubject\",\"GPA\",\"schooltype\",\"intendedgradyear\",\"female\",\"MinorityRace\",\"international\",\"sports\",\"earlyAppl\",\"alumni\",\"outofstate\",\"acceptrate\",\"size\",\"public\",\"finAidPct\",\"instatePct\"]\n",
    "dfpredict = dfr[predictor_cols]\n",
    "dfresponse = dfr[\"acceptStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'admissionstest', u'AP', u'averageAP', u'SATsubject', u'GPA',\n",
       "       u'schooltype', u'intendedgradyear', u'female', u'MinorityRace',\n",
       "       u'international', u'sports', u'earlyAppl', u'alumni', u'outofstate',\n",
       "       u'acceptrate', u'size', u'public', u'finAidPct', u'instatePct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpredict.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates that if we remove all the rows with ANY nulls in it, we reduce our dataset from 13k to 11k. This reduces our dataset too much. So we will impute the missing values. We initially tried to do this using the `mice` package in R, but there does not seem to be an equivalent in Python. Since the % of nulls is just 4%, it shouldn't matter too much what method we use. Since some of the variables are factor, not numerical, we can't use mean or media. We are looking into KNN imputation, but for the time being, just use median. As stated, it shouldn't matter too much what method we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10917, 19)\n",
      "(13310, 19)\n"
     ]
    }
   ],
   "source": [
    "print dfpredict.dropna(axis=0,how=\"any\").shape\n",
    "print dfpredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13310, 19), (13310,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = Imputer(missing_values=\"NaN\", strategy=\"median\", axis=1)\n",
    "imp.fit(dfpredict)\n",
    "X = imp.transform(dfpredict)\n",
    "y = dfresponse\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a training and test data set. The training data will be used to fit the model and the test data to test the model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10648, 19) (2662, 19) (10648,) (2662,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "#X_train, X_validate, y_train, y_validate = train_test_split(X1, y1, test_size=0.25)\n",
    "#print X_train.shape, X_validate.shape, X_test.shape, y_train.shape, y_validate.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "Let's start with evaluating our starting point. What accuracy do we get with the uttermost simple classifier: predicting that all students get accepted (or all students get rejected). Whichever of these is the highest, will be our accuracy to beat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when predicting all accepted =  0.390683696469\n",
      "Accuracy when predicting all rejected =  0.609316303531\n"
     ]
    }
   ],
   "source": [
    "# predict all accepted\n",
    "print 'Accuracy when predicting all accepted = ', metrics.accuracy_score(y_test, np.ones(y_test.shape))\n",
    "# predict all rejected\n",
    "print 'Accuracy when predicting all rejected = ', metrics.accuracy_score(y_test,np.zeros(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is thus an accuracy of 60%, when we predict that no one gets accepted to their universities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We start with a logistic regression. We use cross validation to choose the hyperparameter c. The score for each c is the mean of the scores from the cross validation folds and is stored in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdict = {}\n",
    "for c in [0.001,0.01,0.1,1,10,100,1000]:\n",
    "    clf = linear_model.LogisticRegression(C=c)\n",
    "    #clf.fit(X_train,y_train)\n",
    "    #predicted = clf.predict(X_validate)\n",
    "    #cdict[c] = metrics.accuracy_score(y_validate, predicted)\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    cdict[c] = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the c value with highest accuracy and fit the model using this value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.71299907161774467, 100: 0.66247381895884472, 0.1: 0.68689115434740899, 1000: 0.62950941282578832, 10: 0.69909957947150569, 0.001: 0.6221826024252608, 0.01: 0.6221826024252608}\n",
      "Best parameter:  1 0.712999071618\n",
      "Accuracy on test set:  0.661908339594\n"
     ]
    }
   ],
   "source": [
    "print cdict\n",
    "best_c = max(cdict, key=cdict.get)\n",
    "print 'Best parameter: ', best_c, cdict[best_c]\n",
    "clf = linear_model.LogisticRegression(C=best_c)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ',metrics.accuracy_score(y_test, predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got ~66% accuracy. Not bad, but different models may be able to do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Now we try a random forest, and follow the same procedure as before to choose the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntdict = {}\n",
    "for nt in [100,250,500,1000,2500]:\n",
    "    clf = RandomForestClassifier(n_estimators=nt,criterion=\"gini\")\n",
    "    #clf.fit(X_train,y_train)\n",
    "    #predicted = clf.predict(X_validate)\n",
    "    #ntdict[nt] = metrics.accuracy_score(y_validate, predicted)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    ntdict[nt] = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1000: 0.73140600295053559, 500: 0.73206319173850054, 250: 0.73065456461959477, 100: 0.7273676062953579, 2500: 0.73103006326671471}\n",
      "Best parameter:  500 0.732063191739\n",
      "Accuracy on test set:  0.732907588279\n"
     ]
    }
   ],
   "source": [
    "print ntdict\n",
    "best_nt = max(ntdict, key=ntdict.get)\n",
    "print 'Best parameter: ',best_nt, ntdict[best_nt]\n",
    "clf = RandomForestClassifier(n_estimators=nt)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ',metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the random forst, but now using `entropy` instead of `gini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1000: 0.73337823086948184, 2500: 0.73149950273111985, 250: 0.73253210195886453, 500: 0.73187566293329098, 5000: 0.73290835036837587}\n",
      "Best parameter:  1000 0.733378230869\n",
      "Accuracy on test set:  0.736664162284\n"
     ]
    }
   ],
   "source": [
    "ntdict = {}\n",
    "for nt in [250,500,1000,2500,5000]:\n",
    "    clf = RandomForestClassifier(n_estimators=nt,criterion=\"entropy\")\n",
    "    #clf.fit(X_train,y_train)\n",
    "    #predicted = clf.predict(X_validate)\n",
    "    #ntdict[nt] = metrics.accuracy_score(y_validate, predicted)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    ntdict[nt] = scores.mean()\n",
    "print ntdict\n",
    "best_nt = max(ntdict, key=ntdict.get)\n",
    "print 'Best parameter: ',best_nt, ntdict[best_nt]\n",
    "clf = RandomForestClassifier(n_estimators=nt)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ',metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears `entropy` is pretty much the same as `gini`. This model gets about 73% accuracy, so is clearly better than logistic regression. \n",
    "Let's take a look at feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfpredict.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 0 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-17a20ae65437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfeature_importance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot sorted importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18 is out of bounds for axis 0 with size 18"
     ]
    }
   ],
   "source": [
    "# create dictionary of feature importances\n",
    "feature_importance = {}\n",
    "for i in range(len(dfpredict.columns)):\n",
    "    feature_importance[dfpredict.columns[i]] = clf.feature_importances_[i]\n",
    "# plot sorted importances\n",
    "sns.set_style(\"white\")\n",
    "plt.barh(range(len(dfpredict.columns)),sorted(feature_importance.values()), align=\"center\",alpha=0.7,edgecolor='white')\n",
    "plt.ylim([0,20])\n",
    "plt.yticks(range(len(dfpredict.columns)), sorted(feature_importance,key=feature_importance.get))\n",
    "plt.ylabel(\"feature\")\n",
    "plt.xlabel(\"importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.gca().xaxis.grid(True,color='0.95')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that the admission test scores, the school's acceptance rate and the GPA score are the most important factors in the decision. This is as we expected and what most people think. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Below is a decision tree classifier. Its performance will probably be worse than that of the random forest, as it is in some sense just a subset of a random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test test: 0.64199849737\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "predicted = dt.predict(X_test)\n",
    "print 'Accuracy on test test:', metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one got us only 68% accuracy, so again significantly worse than the random forest. So at this stage, random forest is still the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours\n",
    "Now, let's see how a K-nearest neighbour classifier does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 0.67543297675516067, 10: 0.69167993084544521, 15: 0.69656335381948808, 20: 0.69863005179976057, 25: 0.69656388306352901, 30: 0.6974089093823943}\n",
      "Best parameter:  20 0.6986300518\n",
      "Accuracy on test set:  0.708865514651\n"
     ]
    }
   ],
   "source": [
    "kdict = {}\n",
    "for k in [5,10,15,20,25,30]:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    kdict[k] = scores.mean()\n",
    "print kdict\n",
    "best_k = max(kdict, key=kdict.get)\n",
    "print 'Best parameter: ',best_k, kdict[best_k]\n",
    "clf = KNeighborsClassifier(n_neighbors=best_k)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ', metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph = clf.kneighbors_graph(X_train)\n",
    "#nrows, _ = graph.shape\n",
    "#X_train\n",
    "#for i in range(nrows):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method performs second best to our random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Finally, we try a Linear SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.57673659303558944, 100: 0.57333337743700341, 0.1: 0.43021330740037528, 1000: 0.52514173816974175, 10: 0.52497683454728683, 0.001: 0.57342731825428861, 0.01: 0.52488262910798122}\n",
      "Best parameter:  1 0.576736593036\n",
      "Accuracy on test set:  0.609316303531\n"
     ]
    }
   ],
   "source": [
    "cdict = {}\n",
    "for c in [0.001,0.01,0.1,1,10,100,1000]:\n",
    "    clf = svm.LinearSVC(loss=\"hinge\",C=c);\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cdict[c] = scores.mean()\n",
    "print cdict\n",
    "best_c = max(cdict, key=cdict.get)\n",
    "print 'Best parameter: ', best_c, cdict[best_c]\n",
    "clf = svm.LinearSVC(loss=\"hinge\",C=best_c);\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ',metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is our worst predictor, giving only 61% accuracy. This is barely better than the baseline, but this is maybe not that surprising. We should try a kernelized SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.71299907161774467, 100: 0.66247381895884472, 0.1: 0.68689115434740899, 1000: 0.62950941282578832, 10: 0.69909957947150569, 0.001: 0.6221826024252608, 0.01: 0.6221826024252608}\n",
      "Best parameter:  1 0.712999071618\n",
      "Accuracy on test set:  0.718256949662\n"
     ]
    }
   ],
   "source": [
    "cdict = {}\n",
    "for c in [0.001,0.01,0.1,1,10,100,1000]:\n",
    "    clf = svm.SVC(C=c);\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cdict[c] = scores.mean()\n",
    "print cdict\n",
    "best_c = max(cdict, key=cdict.get)\n",
    "print 'Best parameter: ',best_c, cdict[best_c]\n",
    "clf = svm.SVC(C=best_c);\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ',metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF SVM is doing very well! However, random forest is still better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{200: 0.72004167796823215, 50: 0.71863331547134701, 100: 0.72116901187932358, 250: 0.72051164667667822, 150: 0.72088714532379805}\n",
      "Best parameter:  100 0.721169011879\n",
      "Accuracy on test set:  0.727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ndict={}\n",
    "for n in [50,100,150,200,250]:\n",
    "    clf = AdaBoostClassifier(n_estimators=n);\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    ndict[n] = scores.mean()\n",
    "print ndict\n",
    "best_n = max(ndict, key=ndict.get)\n",
    "print 'Best parameter: ',best_n, ndict[best_n]\n",
    "clf = AdaBoostClassifier(n_estimators=best_n);\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ', metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting closer to our random forest accuracy, but in the end, it looks like random forest is the one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dropping `Intended Graduation Year` As A Predictor\n",
    "\n",
    "Given that our data stretches back only as far as 2009 intended graduation year, it is worth seeing whether dropping `intendedgradyear` as a predictor will help. This assumes that the admissions regime at all of the top 25 colleges has not drastically changed over the last few years. \n",
    "\n",
    "The reason we are dropping this is because all people who use the site will input an intended graduation year of >2018, whereas all of our data has intended graduation year of 2009-2018. So using this as a feature would be extrapolation by definition, so is not entirely reliable. When we test the model on the test data, this data also has intended graduation year from 2009-2018, so this is not extrapolation. But using it as a feature in prediction is not ok, since it is extrapolation. \n",
    "\n",
    "Also, the extra few percentage of accuracy when we use intended graduation year may be overfitting on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{100: 0.72943306937286789, 5: 0.69327648370259132, 1000: 0.73319092258262264, 10: 0.70736262258063809, 50: 0.7237047523909701, 500: 0.73009131664891491, 25: 0.7205105440849261, 250: 0.73187566293329109}\n",
      "1000 0.733190922583\n",
      "0.73328324568\n"
     ]
    }
   ],
   "source": [
    "dfpredict1 = dfpredict.drop(\"intendedgradyear\",axis=1)\n",
    "X = imp.transform(dfpredict1)\n",
    "y = dfresponse\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "ntdict = {}\n",
    "for nt in [5,10,25,50,100,250,500,1000]:\n",
    "    clf = RandomForestClassifier(n_estimators=nt,criterion=\"gini\")\n",
    "    #clf.fit(X_train,y_train)\n",
    "    #predicted = clf.predict(X_validate)\n",
    "    #ntdict[nt] = metrics.accuracy_score(y_validate, predicted)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    ntdict[nt] = scores.mean()\n",
    "print ntdict\n",
    "best_nt = max(ntdict, key=ntdict.get)\n",
    "print 'Best parameter: ',best_nt, ntdict[best_nt]\n",
    "clf = RandomForestClassifier(n_estimators=nt)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print 'Accuracy on test set: ', metrics.accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'college': 'Princeton', 'prob': 0.24399999999999999}, {'college': 'Harvard', 'prob': 0.20000000000000001}, {'college': 'Yale', 'prob': 0.19500000000000001}, {'college': 'Columbia', 'prob': 0.318}, {'college': 'Stanford', 'prob': 0.28299999999999997}, {'college': 'UChicago', 'prob': 0.41399999999999998}, {'college': 'MIT', 'prob': 0.215}, {'college': 'Duke', 'prob': 0.51100000000000001}, {'college': 'UPenn', 'prob': 0.30299999999999999}, {'college': 'CalTech', 'prob': 0.39200000000000002}, {'college': 'JohnsHopkins', 'prob': 0.56725000000000003}, {'college': 'Dartmouth', 'prob': 0.48699999999999999}, {'college': 'Northwestern', 'prob': 0.65389999999999993}, {'college': 'Brown', 'prob': 0.255}, {'college': 'Cornell', 'prob': 0.63200000000000001}, {'college': 'Vanderbilt', 'prob': 0.68799999999999994}, {'college': 'WashU', 'prob': 0.64424999999999999}, {'college': 'Rice', 'prob': 0.71299999999999997}, {'college': 'NotreDame', 'prob': 0.70199999999999996}, {'college': 'UCB', 'prob': 0.83299999999999996}, {'college': 'Emory', 'prob': 0.67600000000000005}, {'college': 'Georgetown', 'prob': 0.60799999999999998}, {'college': 'CarnegieMellon', 'prob': 0.77500000000000002}, {'college': 'UCLA', 'prob': 0.78900000000000003}, {'college': 'USC', 'prob': 0.74199999999999999}]\n"
     ]
    }
   ],
   "source": [
    "### test a candidate's chances for each college\n",
    "\n",
    "candidate = [0.926899206,7,1.06733864,\n",
    "             3,-0.187109979,0,1,\n",
    "             0,0,0,0,0,\n",
    "             0]\n",
    "\n",
    "college_cols = [\"acceptrate\",\"size\",\"public\",\"finAidPct\",\"instatePct\"]\n",
    "colleges = ti.College()\n",
    "preds = []\n",
    "for i, row in colleges.df.iterrows():\n",
    "    Xp1 = candidate + list(row[college_cols])\n",
    "    y = clf.predict_proba(Xp1)[0][1]\n",
    "    p =  {'college':row.collegeID, 'prob':y}\n",
    "    preds.append(p)\n",
    "    \n",
    "print preds   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
