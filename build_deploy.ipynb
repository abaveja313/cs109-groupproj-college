{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B - Building and Deploying the App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - The Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The code for the webservice can be found in the ./webservice subdirectory. This is a copy of the code running in production. The reference origin git repo is *not* in this repository. It is hosted on [Heroku](https://www.heroku.com/). Ask if you want access to the production git repository.\n",
    "\n",
    "### How the Webservice Works\n",
    "\n",
    "The webservice is a minimal [Flask](http://flask.pocoo.org/) application used to provide prediction probabilities for a given candidate. There is one useful entry point: `/predict` that expects the user's admissionTest, AP, etc on the query string. It will return a JSON file consisting of the probabilities of getting into each college.\n",
    "\n",
    "Sample input:\n",
    "\n",
    "```\n",
    "http://mypythonapp-wihl.rhcloud.com/predict?admissionstest=0.926899206&AP=7&averageAP=1.06733864&SATsubject=0.324271565&GPA=-0.187109979&schooltype=0&intendedgradyear=2017&female=1&MinorityRace=0&international=0&sports=0&earlyAppl=0&alumni=0&outofstate=0&acceptrate=0.151&size=6621&public=0&finAidPct=0&instatePct=0\n",
    "```\n",
    "\n",
    "Sample output:\n",
    "```\n",
    "{\n",
    "  \"preds\": [\n",
    "    {\n",
    "      \"college\": \"Princeton\",\n",
    "      \"prob\": 0.26166666666666666\n",
    "    },\n",
    "    {\n",
    "      \"college\": \"Harvard\",\n",
    "      \"prob\": 0.23999999999999999\n",
    "    },\n",
    "    {\n",
    "      \"college\": \"Yale\",\n",
    "      \"prob\": 0.23999999999999999\n",
    "    },\n",
    "    ...\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Webservice startup\n",
    "\n",
    "Upon getting the first `/predict` request, the web service will perform the same logic as the classification iPython notebook. It loads the normalized college data, imputes missing values, and runs Scikit-Learn's Random Forest classification. The resulting classifier is kept in memory as a Python global variable to service subsequent prediction requests. There is no locking at the present time. \n",
    "\n",
    "### Webservice Dependencies - OpenShift\n",
    "\n",
    "We started using OpenShift. The free account worked fine at first and then had consistent and terrible performance problems a few days before the project was due. With two days remaining, we scrambled and moved from OpenShift to Heroku. This section serves as reference (and consider it a warning to not use OpenShit ever again).\n",
    "\n",
    "Since the webservice is running the full Pandas and Scikit-Learn stacks, these had to installed on the OpenShift cartridge. Here's what was done:\n",
    "\n",
    "1. Create an OpenShift account\n",
    "1. Install the [client tools](https://developers.openshift.com/en/managing-client-tools.html). This will install `rhc`, the necessary local command line tool for managing OpenShift apps.\n",
    "1. Use the Flask Quickstart template ([details](https://developers.openshift.com/en/python-flask.html))\n",
    "\n",
    "    ```\n",
    "    rhc app create myflaskapp python-2.7 --from-code=https://github.com/openshift-quickstart/flask-base.git\n",
    "    ```\n",
    "\n",
    "1. This will create a local myflaskapp git repository. Go into this repository: `cd myflaskapp`\n",
    "1. SSH into the app and install the dependent packages:\n",
    "\n",
    "    ```\n",
    "    rhc ssh myflaskapp\n",
    "    source ~/python/virtenv/activate\n",
    "    pip install numpy\n",
    "    ```\n",
    "\n",
    "    The `pip install` has to be repeated for `scipy, pandas` and `scikit-learn`. This takes a while as it is compiled     locally on the OpenShift instance and may not have optimal performance.\n",
    "\n",
    "1. After all the packages have been installed, take the output of `pip freeze` and update the `requirements.txt` in the *local* repository.\n",
    "1. At this point, you can grab the appropriate files from `./webservice` directory, notably: `TIdatabase.py, collegelist.csv, collegedata_normalized.csv, flaskapp.py`.\n",
    "\n",
    "\n",
    "\n",
    "#### DevOps Notes\n",
    "\n",
    "To see the logs, use `rhc tail -o '-n 100' mypythonapp`\n",
    "\n",
    "Common rhc commands can be found [here](https://developers.openshift.com/en/managing-common-rhc-commands.html)\n",
    "\n",
    "### Webservice Dependencies - Heroku\n",
    "\n",
    "Heroku was easier to configure since there are buildpacks available that contain the entire Condas stack with all the Scipy, Numpy, Scikit-learn dependencies. There were still numerous gotchas, mainly related to finding the right\n",
    "combination of scipy, numpy and scikit-learn versions that would all play nicely together.\n",
    "\n",
    "Heroku has a [nice walkthrough](https://devcenter.heroku.com/articles/getting-started-with-python#introduction) about setting up a Python app in minutes. I mostly followed that, with the following changes:\n",
    "\n",
    "Add the Conda buildpack:\n",
    "```\n",
    "heroku config:add BUILDPACK_URL=https://github.com/kennethreitz/conda-buildpack.git\n",
    "```\n",
    "\n",
    "This buildpack has a broken scipy, so it was obtained from:\n",
    "```\n",
    "heroku buildpacks:set https://github.com/thenovices/heroku-buildpack-scipy\n",
    "```\n",
    "\n",
    "This version scipy is broken with the latest scikit-learn, so I had to downgrade scikit-learn. Here is our final requirements.txt file:\n",
    "```\n",
    "gunicorn==19.3.0\n",
    "psycopg2==2.6\n",
    "SQLAlchemy==1.0.4\n",
    "whitenoise==1.0.6\n",
    "Flask==0.10.1\n",
    "pandas==0.17.1\n",
    "numpy==1.9.1\n",
    "scipy==0.15.1\n",
    "scikit-learn==0.16.1\n",
    "nose==1.3.7\n",
    "```\n",
    "\n",
    "and our Procfile\n",
    "```\n",
    "web: gunicorn flaskapp:app --log-file=-\n",
    "```\n",
    "\n",
    "After the pain and suffering with OpenShift's free account, we went with the Heroku \\$7/mo hobbyist dyno with the hope that the app would not go down again.\n",
    "\n",
    "### Programming Notes\n",
    "\n",
    "Using the Quickstart template, the real work is done in `flaskapp.py`.\n",
    "\n",
    "Logging is off by default. To log errors from your app, use:\n",
    "\n",
    "```\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "```\n",
    "\n",
    "We simply used global variables to stored information. It is also possible to use the [appcontext](http://flask.pocoo.org/docs/0.10/appcontext/). \n",
    "\n",
    "Note that the app can be tested locally very easily. From a local shell use: `python flaskapp.py`. It will say which address / port it is listening on when starting up.\n",
    "\n",
    "### Consuming the Webservice from R\n",
    "\n",
    "Sample code to consume the webservice can be found in `rclient.R`. This simulates how the production Shiny app can invoke the webservice. An R data.frame is created with the normalized user inputted values. This is used to populate the query string of the webservice. Note that the webservice ignores the last five variables, which are specific to a given college, since probabilities for *all* colleges are returned.\n",
    "\n",
    "The returned JSON is easily parsed into an R data.frame for presentation to the user or further manipulation. Here is a snippet:\n",
    "\n",
    "```\n",
    "# create query string\n",
    "qs = paste0(colnames(pred),\"=\",pred[1,],collapse=\"&\")\n",
    "server = \"http://127.0.0.1:5000/predict\"\n",
    "server = \"http://mypythonapp-wihl.rhcloud.com/predict\"\n",
    "\n",
    "URL = paste0(server,\"?\",qs)\n",
    "\n",
    "js  = fromJSON(URL)\n",
    "df = js$preds\n",
    "df$college = as.factor(df$college)\n",
    "summary(df)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Webservice Code\n",
    "\n",
    "(This is not in a code cell because it is not meant to be executed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import TIdatabase as ti\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "clf = None\n",
    "logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    " \n",
    "\n",
    "ws_cols = [\"admissionstest\",\"AP\",\"averageAP\",\"SATsubject\",\"GPA\",\"schooltype\",\n",
    "                  \"female\",\"MinorityRace\",\"international\",\"sports\",\n",
    "                  \"earlyAppl\",\"alumni\",\"outofstate\"]\n",
    "college_cols = [\"acceptrate\",\"size\",\"public\"]\n",
    "predictor_cols = ws_cols + college_cols\n",
    "\n",
    "cols_to_drop = ['classrank', 'canAfford', 'firstinfamily', 'artist', 'workexp', 'visited', 'acceptProb',\n",
    "                'addInfo','intendedgradyear']\n",
    "NUM_ESTIMATORS = 1000\n",
    "\n",
    "colleges = ti.College()\n",
    "\n",
    "def load_classifier():\n",
    "    global clf\n",
    "    df = pd.read_csv(os.path.join(os.path.dirname(__file__),\"collegedata_normalized.csv\"), index_col=0)\n",
    "    dfr = df.drop(cols_to_drop,axis=1)\n",
    "    dfr = dfr[pd.notnull(df[\"acceptStatus\"])]\n",
    "    dfpredict = dfr[predictor_cols]\n",
    "    dfresponse = dfr[\"acceptStatus\"]\n",
    "    imp = Imputer(missing_values=\"NaN\", strategy=\"median\", axis=1)\n",
    "    imp.fit(dfpredict)\n",
    "    X = imp.transform(dfpredict)\n",
    "    y = dfresponse\n",
    "    clf = RandomForestClassifier(n_estimators=NUM_ESTIMATORS, criterion=\"gini\")\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def genPredictionList(vals):\n",
    "    \"\"\"\n",
    "    vals (coming from the request arguments) is a list of tuples [('name1','val1'),('name2','val2')...]\n",
    "    \"\"\"\n",
    "    global ws_cols\n",
    "    global clf\n",
    "    global colleges\n",
    "    X = pd.Series(dict((name, float(val)) for name, val in vals))\n",
    "    if clf is None: load_classifier()\n",
    "    preds = []\n",
    "    for i, row in colleges.df.iterrows():\n",
    "        X[college_cols] = row[college_cols]\n",
    "        y = clf.predict_proba(X[predictor_cols])[0][1]\n",
    "        p = {'college':row.collegeID, 'prob':y}\n",
    "        preds.append(p)\n",
    "    return preds\n",
    "    #e.g.  [{'college':'harvard', 'prob':y}, {'college':'yale', 'prob':0.25}, {'college':'brown', 'prob':0.89}]\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return \"Welcome to the Team Ivy Web Service\"\n",
    "\n",
    "@app.route(\"/predict\")\n",
    "def predict():\n",
    "    preds = genPredictionList(request.args.iteritems())\n",
    "    return jsonify(preds = preds)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2 - The Shiny App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shiny](http://shiny.rstudio.com/) is a web application framework for R. It allows rapid development of reactive web applications. In this project, Shiny is used to implement all user interaction including plots and charts.\n",
    "\n",
    "The Shiny app is hosted at http://www.shinyapps.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - SquareSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SquareSpace hosts the static portion of the public facing web site. It also provides summary usage statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "Getting Started with Python on Heroku https://devcenter.heroku.com/articles/getting-started-with-python-o#prerequisites\n",
    "\n",
    "Buildpack for Conda on Heroku https://github.com/kennethreitz/conda-buildpack\n",
    "\n",
    "Getting started with OpenShift and Python 2.7 (without Flask): https://developers.openshift.com/en/python-getting-started.html\n",
    "\n",
    "Getting started with OpenShift and Flask: https://developers.openshift.com/en/python-flask.html\n",
    "\n",
    "Blog post about OpenShift and Flask https://blog.openshift.com/day-3-flask-instant-python-web-development-with-python-and-openshift/\n",
    "\n",
    "Somewhat dated: https://blog.openshift.com/beginners-guide-to-writing-flask-apps-on-openshift/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
